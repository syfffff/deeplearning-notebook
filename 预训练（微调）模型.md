# 预训练（微调）模型

## 1.预训练模型 

```python
#使用原模型  pretrained = True 是说我们将模型下载下来 并且把他的超参数拿过来
pretrained_net = torchvision.models.resnet18(pretrained=True)

pretrained_net.fc

#构建一个全新的网络模型resnet18 改变全连接层
finetune_net = torchvision.models.resnet18(pretrained=True)
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)
#只对最后一层的weight做随机初始化
nn.init.xavier_uniform_(finetune_net.fc.weight)
```

## 2.在最后一层使用了更大的[学习率](https://so.csdn.net/so/search?q=学习率&spm=1001.2101.3001.7020)让网络训练最后一层全连接层

```python
def train_fine_tuning(net, learning_rate,
                      batch_size=128, num_epochs=5, param_group=True):
    #将所有的trian 和test导入
    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_augs),batch_size=batch_size, shuffle=True)
    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_augs),batch_size=batch_size)
    devices = d2l.try_all_gpus()
    #这个函数默认的是mean，他的意思是在一个batch将所有的损失值做reduction
    #sum是求和   none是啥都不做
    loss = nn.CrossEntropyLoss(reduction="none")
    #如果param_group等于ture
    if param_group:
        #如果不是fc.weight 和bais 就将他赋给params_1x
        params_1x = []
        for name,param in net.named_parameters():
            if name not in['fc.weight','fc.bias']:
                params_1x.append(param)
        #原来的params就放入做微调，如果是最后一个全连接层 就将它的学习率*10  让他快速收敛
        trainer = torch.optim.SGD([{'params': params_1x},
                                   {'params': net.fc.parameters(),
                                    'lr': learning_rate * 10}],
                                lr=learning_rate, weight_decay=0.001)
    else:
        #如果是fulse 就执行原来的
        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,
                                  weight_decay=0.001)
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,
                   devices)
#使用微调的现模型
train_fine_tuning(finetune_net, 5e-5)
```

