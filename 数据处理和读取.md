# csv处理

## 1.读取csv

```python

import pandas as pd
train_csv = pd.read_csv('train.csv')
test_csv = pd.read_csv('test.csv')
train_csv.head()
```

train_csv.head()是pandas中的一个函数，用于显示DataFrame的前几行，默认显示前5行。这个函数通常用于数据集的快速预览，以便了解数据的大致结构和内容。

|      |        image |      label       |
| ---: | -----------: | :--------------: |
|    0 | images/0.jpg | maclura_pomifera |
|    1 | images/1.jpg | maclura_pomifera |
|    2 | images/2.jpg | maclura_pomifera |
|    3 | images/3.jpg | maclura_pomifera |
|    4 | images/4.jpg | maclura_pomifera |

## 2.分割csv

```
train_csv.loc[:, 'label']

```

train_csv.loc[:, 'label']的意思是选择train_csv表中的所有行（:表示所有行），并且选择label列。这是pandas中的一种用法，可以通过loc方法来选择数据集的特定行和列。这里的“:”表示选择所有的行，而“label”表示指定选择的列名。

## 3.查看有多少种类别并且给予标号



```python
train_csv.loc[:, 'label'].unique()#等价于train_csv.label.unique()
```

选择train_csv表中的所有行，然后选择label列，并使用unique()方法获取该列的唯一值。这个方法会返回一个包含该列所有唯一值的numpy数组。在这个例子中，它返回的是训练集中所有不同的标签值（labels）

```python
class_to_num = dict(zip(list(train_csv.loc[:, 'label'].unique()), range(len(train_csv.label.unique())) ))
print(class_to_num)
print(len(class_to_num))
```

len(train_csv.label.unique())为176 也就是要分类176类

list(train_csv.loc[:, 'label'].unique())将包含唯一标签的numpy数组转换为列表，这个列表中包含训练集中所有不同的标签值（labels）。range(len(train_csv.label.unique()))生成一个从0到标签数量-1的数字序列，这里的标签数量是通过train_csv.label.unique()得到的。

使用zip函数将标签字符串和数字序列一一对应，得到一个字典。这个字典中，标签字符串是键，数字序列是值，可以将标签字符串映射为数字，方便后续的数据处理和模型训练。

````
a = [1, 2, 3]
b = ['a', 'b', 'c']
zipped = zip(a, b)
```
此时，zipped是一个zip对象，可以使用list()函数将其转换为一个包含元组的列表：

```
[(1, 'a'), (2, 'b'), (3, 'c')]
````

```
`{'maclura_pomifera': 0, 'ulmus_rubra': 1, 'broussonettia_papyrifera': 2, 'prunus_virginiana': 3, 'acer_rubrum': 4,`
```

# 图片读取

## 4.图片路径

```python
import os
#path='\images'

image1_path = os.path.join( train_csv.loc[0, 'image'])
print(image1_path)
```

​        images/0.jpg

这段代码的作用是构造训练集中第一张图像的完整文件路径。具体来说，代码中首先定义了一个变量path，表示数据集所在的目录路径。然后，使用os.path.join()函数将path和训练集中第一张图像的文件名拼接起来，得到图像文件的完整路径。

在代码中，train.loc[0, 'image']表示训练集中第一行数据的'image'列，即第一张图像的文件名。os.path.join(path, train.loc[0, 'image'])将path和文件名拼接起来，得到形如'/kaggle/input/classify-leaves/train/100001.jpg'的完整文件路径，保存在image1_path变量中。

os.path.join()是Python内置的用于路径拼接的函数，可以将多个路径组合成一个完整的路径。

当path为空时候，默认为本身文件夹路径

## 5显示图片

```python
import matplotlib.pyplot as plt
from PIL import Image

image1 = Image.open(image1_path)
plt.imshow(image1)
plt.title(train.loc[0, 'label'])
plt.axis('off')
plt.show()
```

这段代码的作用是使用matplotlib库和PIL库显示训练集中第一张图像。具体来说，代码中首先使用PIL库的Image.open()函数读取指定路径中的图像文件，得到一个PIL图像对象image1。

然后，用matplotlib库的plt.imshow()函数显示这个图像。plt.imshow()函数将图像数据显示为图像，并可以添加标题、轴标签等元素。在这个例子中，使用plt.imshow()函数显示image1图像，然后使用plt.title()函数添加图像标题，使用plt.axis()函数设置坐标轴的显示方式，最后使用plt.show()函数显示图像。

这个代码可以用于检查图像文件是否能够正常读取，以及查看图像的内容和标签信息。在实际的项目中，通常需要使用类似的代码对数据集中的所有图像进行遍历和处理。

## 6.图片转tensor

```python
import torchvision.transforms as transform
img2tensor = transform.ToTensor()
print(img2tensor(image1).shape)

torch.Size([3, 224, 224])
```

这段代码的作用是将PIL图像对象转换为PyTorch张量（Tensor）对象，并检查张量的维度。

## 7.对字典 key 和value 颠倒

```python
num_to_class = { a : b for b, a in class_to_num.items()}
print(num_to_class)
```

```
0: 'maclura_pomifera', 1: 'ulmus_rubra', 2: 'broussonettia_papyrifera', 3: 'prunus_virginiana', 4: 'acer_rubrum', 5: 'cryptomeria_japonica', 6: 'staphylea_trifolia',
```

class_to_num.items()是一个字典方法，用于返回字典中的所有键值对。该方法返回一个类似于列表的可迭代对象，其中每个元素都是一个键值对，表示字典中的一个条目。

例如，如果有一个字典class_to_num，其值为{'A': 0, 'B': 1, 'C': 2}，则可以使用class_to_num.items()方法返回这个字典的所有键值对：

```
for key, value in class_to_num.items():
    print(key, value)
```

这个代码会依次输出字典中的每个键值对，即：

```
A 0
B 1
C 2
```

# 数据集创建

## 8创建dataset

```python
class LeavesDataset(Data.Dataset):
    """
    args:
    csv_path: indicates the path of csv,
    file_path: indicates where the images loads,
    mode: choose between 'train', 'valid' and 'test'. default set 'train'.
    valid_ratio: indicates the length of valid dataset. default 0.2.
    resize_height: indicates how to resize the images. default 256.
    resize_width: indicates how to resize the images. default 256.
    """
    def __init__(self, csv_path, file_path, mode='train', valid_ratio=0.2, resize_height=256, resize_width=256):
        self.resize_height = resize_height
        self.resize_width = resize_width
        self.file_path = file_path
        self.mode = mode
        self.data_info = pd.read_csv(csv_path)
        self.data_len = len(self.data_info.index)
        self.train_len = int(self.data_len * (1 - valid_ratio))
        
        if mode == 'train':
            self.train_image = np.asarray(self.data_info.loc[:self.train_len, 'image'])
            self.train_label = np.asarray(self.data_info.loc[:self.train_len, 'label'])
            self.image_arr = self.train_image
            self.label_arr = self.train_label
        elif mode == 'valid':
            self.valid_image = np.asarray(self.data_info.loc[self.train_len:, 'image'])
            self.valid_label = np.asarray(self.data_info.loc[self.train_len:, 'label'])
            self.image_arr = self.valid_image
            self.label_arr = self.valid_label
        elif mode == 'test':
            self.test_image = np.asarray(self.data_info.loc[:, 'image'])
            self.image_arr = self.test_image
            
        self.real_len = len(self.image_arr)
        
        print('Finished reading %s dataset. %d number samples found.' % (mode, self.real_len))
        
        
    def __getitem__(self, index):
        image_path = self.image_arr[index]
        image = Image.open(os.path.join(self.file_path, image_path))
        
        if self.mode == 'train':
            transform = transforms.Compose([transforms.RandomResizedCrop(224),
                                 transforms.RandomHorizontalFlip(),
                                 transforms.ToTensor(),
                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
        else:
            transform = transforms.Compose([transforms.Resize(256),
                               transforms.CenterCrop(224),
                               transforms.ToTensor(),
                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])
            
        image = transform(image)
        
        if self.mode == 'test':
            return image
        
        label = self.label_arr[index]
        label_num = class_to_num[label]
        return image, label_num
    
    
    def __len__(self):
        return self.real_len
```

创建自己的数据集，这是一个用于加载叶子图像数据集的 PyTorch 数据集类，用于将数据集处理成 PyTorch 中的数据集类型。该类的初始化函数接收以下参数：

- csv_path：表示 CSV 文件的路径，其中包含数据集中每个图像的信息及其对应的标签。
- file_path：表示包含图像文件的文件夹路径。
- mode：表示数据集的模式，可以是 'train'、'valid' 或 'test'，默认为 'train'。
- valid_ratio：表示用于验证集的数据的比例，默认为 0.2。
- resize_height：表示将每个图像调整为的高度，默认为 256。
- resize_width：表示将每个图像调整为的宽度，默认为 256。

该类中包含一个 __getitem__ 函数，用于从数据集中获取单个图像及其标签。在训练模式下，该函数会对图像进行一系列的数据增强操作，如随机水平翻转、随机

## 9.dataset实例化

```python
train_path = 'train.csv'
test_path = 'test.csv'
image_path = ''

data_transform = {
    "train": transforms.Compose([transforms.RandomResizedCrop(224),
                                 transforms.RandomHorizontalFlip(),
                                 transforms.ToTensor(),
                                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),
    "val": transforms.Compose([transforms.Resize(256),
                               transforms.CenterCrop(224),
                               transforms.ToTensor(),
                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}

train_dataset = LeavesDataset(train_path, image_path, mode='train', valid_ratio=0.2)
valid_dataset = LeavesDataset(train_path, image_path, mode='valid', valid_ratio=0.2)
test_dataset = LeavesDataset(test_path, image_path, mode='test')
```

创建数据集

```
Finished reading train dataset. 14683 number samples found.
Finished reading valid dataset. 3671 number samples found.
Finished reading test dataset. 8800 number samples found.
```

## 10.创建dataloader迭代器

```python
train_loader = Data.DataLoader(train_dataset, batch_size=5, shuffle=True, num_workers=4)

valid_loader = Data.DataLoader(valid_dataset, batch_size=5, shuffle=True, num_workers=4)

test_loader = Data.DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=4)
```

`__getitem__()` 是在 `DataLoader` 中被调用的。在创建 `DataLoader` 对象时，会将要加载的数据集传递给 `DataLoader`，然后在调用 `DataLoader` 的 `__iter__()` 方法时，会调用数据集的 `__getitem__()` 方法，从而获取单个样本。在 `__iter__()` 方法内部，会将数据集中的样本按照指定的 `batch_size` 分成若干个批次，然后对每个批次的样本调用 `collate_fn()` 函数进行处理，最终返回一个批次的数据，即一个包含 `batch_size` 个样本的张量。

所以，在 `DataLoader` 中的调用顺序是：`__iter__()` -> `__getitem__()` -> `collate

## 11.查看dateloader迭代器

```
def show_loader(loader):
    for (x, y) in loader:
        print(x)
        print(y)
        break
show_loader(valid_loader)
```